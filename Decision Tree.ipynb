{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd07a0831dc404eeb13453f8540f5b7674d5d6ce1ff6abfc97faa9ee3d578e610be",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# What is a Decision Tree?\n",
    "\n",
    "## A Nonparametric Machine Learning Algorithm\n",
    "\n",
    "So far we’ve been dealing with Logistic Regression. In Logistic Regression, we look at the data graphically and draw a line to separate the data. The model is defined by the coefficients that define the line. These coefficients are called parameters. Since the model is defined by these parameters, Logistic Regression is a parametric machine learning algorithm.\n",
    "\n",
    "In this module, we’ll introduce Decision Trees, which are an example of a nonparametric machine learning algorithm. Decision Trees won’t be defined by a list of parameters as we’ll see in the upcoming lessons.\n",
    "\n",
    "> Every machine learning algorithm is either parametric or nonparametric."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tree Terminology\n",
    "\n",
    "The reason many people love decision trees is because they are very easy to interpret. It is basically a flow chart of questions that you answer about a datapoint until you get to a prediction.\n",
    "\n",
    "Here’s an example of a Decision Tree for the Titanic dataset. We’ll see in the next lesson how this tree is constructed.\n",
    "\n",
    "Each of the rectangles is called a node. The nodes which have a feature to split on are called internal nodes. The very first internal node at the top is called the root node. The final nodes where we make the predictions of survived/didn’t survive are called leaf nodes. Internal nodes all have two nodes below them, which we call the node’s children\n",
    "\n",
    "![](images/tree1.png)\n",
    "\n",
    "> The terms for trees (root, leaf) come from an actual tree, though it’s upside down since we generally draw the root at the top. We also use terms that view the tree as a family tree (child node & parent node)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Interpreting a Decision Tree\n",
    "\n",
    "To interpret this Decision Tree, let’s run through an example. Let’s say we want to know the prediction for a 10 year old male passenger in Pclass 2. At the first node, since the passenger’s sex is male, we go to the right child. Then, since their age 10 which is <=13 we go to the left child, and at the third node we go to the right child since the Pclass is 2. In the following diagram we highlight the path for this passenger.\n",
    "\n",
    "![](images/tree2.png)\n",
    "\n",
    "Note that there are no rules that we use every feature, or what order we use the features, or for a continuous value (like Age), where we do the split. It is standard in a Decision Tree to have each split just have 2 options.\n",
    "\n",
    "> Decision Trees are often favored if you have a non-technical audience since they can easily interpret the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How did we get the Decision Tree?\n",
    "\n",
    "When building the Decision Tree, we don’t just randomly choose which feature to split on first. We want to start by choosing the feature with the most predictive power. Let’s look at our same Decision Tree again.\n",
    "\n",
    "![](images/tree3.png)\n",
    "\n",
    "Intuitively for our Titanic dataset, since women were often given priority on lifeboats, we expect the Sex to be a very important feature. So using this feature first makes sense. On each side of the Decision Tree, we will independently determine which feature to split on next. In our example above, the second split for women is on Pclass. The second split for men is on Age. We also note for some cases we do three splits and for some just two.\n",
    "\n",
    "> For any given dataset, there’s a lot of different possible Decision Trees that could be created depending on the order you use the features. In the upcoming lessons, we’ll see how to mathematically choose the best Decision Tree."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# How to build a Decision Tree\n",
    "\n",
    "## What makes a good split\n",
    "\n",
    "In order to determine which feature we should split on first, we need to score every possible split so we can choose the split with the highest score. Our goal would be to perfectly split the data. If, for instance, all women survived the crash and all men didn’t survive, splitting on Sex would be a perfect split. This is rarely going to happen with a real dataset, but we want to get as close to this as possible.\n",
    "\n",
    "The mathematical term we’ll be measuring is called information gain. This will be a value from 0 to 1 where 0 is the information gain of a useless split and 1 is the information gain of a perfect split. In the next couple parts we will define gini impurity and entropy which we will use to define information gain. First we will discuss the intuition of what makes a good split.\n",
    "\n",
    "Let’s consider a couple possible splits for the Titanic dataset. We’ll see how it splits the data and why one is better than the other.\n",
    "\n",
    "First, let’s trying splitting on Age. Since Age is a numerical feature, we need to pick a threshold to split on. Let’s say we split on Age<=30 and Age>30. Let’s see how many passengers we have on each side, and how many of them survived and how many didn’t.\n",
    "\n",
    "![](images/tree4.png)\n",
    "\n",
    "On both sides, we have about 40% of the passengers surviving. Thus we haven’t really gained anything from splitting the data this way.\n",
    "\n",
    "Now let’s try splitting on Sex.\n",
    "\n",
    "![](images/tree5.png)\n",
    "\n",
    "We can see on the female side that the vast majority survived. On the male side, the vast majority didn’t survive. This is a good split.\n",
    "\n",
    "What we’re going for is homogeneity (or purity) on each side. Ideally we would send all the passengers who survived to one side and those who didn’t survive to the other side. We’ll look at two different mathematical measurements of purity. We’ll use the purity values to calculate the information gain.\n",
    "\n",
    "> A good choice of a feature to split on results in each side of the split being pure. A set is pure if all the datapoints belong to the same class (either survived or didn’t survive).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gini Impurity\n",
    "\n",
    "Gini impurity is a measure of how pure a set is. We’ll later see how we can use the gini impurity to calculate the information gain.\n",
    "\n",
    "We calculate the gini impurity on a subset of our data based on how many datapoints in the set are passengers that survived and how many are passengers who didn’t survive. It will be a value between 0 and 0.5 where 0.5 is completely impure (50% survived and 50% didn’t survive) and 0 is completely pure (100% in the same class).\n",
    "\n",
    "The formula for gini is as follows. p is the percent of passengers who survived. Thus (1-p) is the percent of passengers who didn’t survive.\n",
    "\n",
    "![](images/gini_equation.png)\n",
    "\n",
    "Here’s a graph of the gini impurity.\n",
    "\n",
    "![](images/gini_graph.png)\n",
    "\n",
    "We can see that the maximum value is 0.5 when exactly 50% of the passengers in the set survived. If all the passengers survived or didn’t survive (percent is 0 or 1), then the value is 0.\n",
    "\n",
    "Let’s calculate the gini impurity for our examples from the previous part. First we had a split on Age<=30 and Age>30. Let’s calculate the gini impurities of the two sets that we create.\n",
    "\n",
    "![](images/tree4.png)\n",
    "\n",
    "On the left, for the passengers with Age<=30, let’s first calculate the percent of passengers who survived:\n",
    "\n",
    "Percent of passengers who survived = 197/(197+328) = 0.3752\n",
    "Percent of passengers who didn’t survive = 1 - 0.375 = 0.6248\n",
    "\n",
    "Now let’s use that to calculate the gini impurity:\n",
    "\n",
    "```\n",
    "2 * 0.3752 * 0.6248 = 0.4689\n",
    "```\n",
    "\n",
    "We can see that this value is close to 0.5, the maximum value for gini impurity. This means that the set is impure.\n",
    "\n",
    "Now let’s calculate the gini impurity for the right side, passengers with Age>30.\n",
    "\n",
    "```\n",
    "2 * 145/(145+217) * 217/(145+217) = 0.4802\n",
    "```\n",
    "\n",
    "This value is also close to 0.5, so again we have an impure set.\n",
    "\n",
    "Now let’s look at the gini values for the other split we tried, splitting on Sex.\n",
    "\n",
    "![](images/tree5.png)\n",
    "\n",
    "On the left side, for female passengers, we calculate the following value for the gini impurity.\n",
    "\n",
    "```\n",
    "2 * 233/(233+81) * 81/(233+81) = 0.3828\n",
    "```\n",
    "\n",
    "On the right side, for male passengers, we get the following value.\n",
    "\n",
    "```\n",
    "2 * 109/(109+464) * 464/(109+464) = 0.3081\n",
    "```\n",
    "\n",
    "Both of these values are smaller than the gini values for splitting on Age, so we determine that splitting on the Sex feature is a better choice.\n",
    "\n",
    "> Right now we have two values for each potential split. The information gain will be a way of combining them into a single value.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy is another measure of purity. It will be a value between 0 and 1 where 1 is completely impure (50% survived and 50% didn’t survive) and 0 is completely pure (100% the same class).\n",
    "\n",
    "The formula for entropy comes from physics. p again is the percent of passengers who survived.\n",
    "\n",
    "![](images/entropy_equation.png)\n",
    "\n",
    "Here’s a graph of the entropy function.\n",
    "\n",
    "![](images/entropy_graph.png)\n",
    "\n",
    "You can see it has a similar shape to the gini function. Like the gini impurity, the maximum value is when 50% of the passengers in our set survived, and the minimum value is when either all or none of the passengers survived. The shape of the graphs are a little different. You can see that the entropy graph is a little fatter.\n",
    "\n",
    "Now let’s calculate the entropy values for the same two potential splits.\n",
    "\n",
    "![](images/tree4.png)\n",
    "\n",
    "```\n",
    "On the left (Age<=30):\n",
    "p = 197/(197+328) = 0.3752\n",
    "Entropy = -(0.375 * log(0.375) + (1-0.375) * log(1-0.375)) = 0.9546\n",
    "```\n",
    "\n",
    "```\n",
    "On the left (Age<=30):\n",
    "p = 197/(197+328) = 0.3752\n",
    "Entropy = -(0.375 * log(0.375) + (1-0.375) * log(1-0.375)) = 0.9546\n",
    "```\n",
    "\n",
    "These values are both close to 1, which means the sets are impure.\n",
    "\n",
    "Now let’s do the same calculate for the split on the Sex feature.\n",
    "\n",
    "![](images/tree5.png)\n",
    "\n",
    "```\n",
    "On the left (female):\n",
    "p = 233/(233+81) = 0.7420\n",
    "Entropy = -(p * log(p) + (1-p) * log(1-p)) = 0.8237\n",
    "```\n",
    "\n",
    "```\n",
    "And on the right (male):\n",
    "p = 109/(109+464) = 0.1902\n",
    "Entropy =  -(p * log(p) + (1-p) * log(1-p)) = 0.7019\n",
    "```\n",
    "\n",
    "You can see that these entropy values are smaller than the entropy values above, so this is a better split.\n",
    "\n",
    "> It’s not obvious whether gini or entropy is a better choice. It often won’t make a difference, but you can always cross validate to compare a Decision Tree with entropy and a Decision Tree with gini to see which performs better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(a, b):\n",
    "    p = a/(a+b)\n",
    "\n",
    "    return 2 * p * (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4738112457404905"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "gini(342, 545)"
   ]
  },
  {
   "source": [
    "## Information Gain\n",
    "\n",
    "Now that we have a way of calculating a numerical value for impurity, we can define information gain.\n",
    "\n",
    "![](images/information_gain_equation.png)\n",
    "\n",
    "H is our impurity measure (either Gini impurity or entropy). S is the original dataset and A and B are the two sets we’re splitting the dataset S into. In the first example above, A is passengers with Age<=30 and B is passengers with Age>30. In the second example, A is female passengers and B is male passengers. |A| means the size of A.\n",
    "\n",
    "Let’s calculate this value for our two examples. Let’s use Gini impurity as our impurity measure.\n",
    "\n",
    "We’ve already calculated most of the Gini impurity values, though we need to calculate the Gini impurity of the whole set. There are 342 passengers who survived and 545 passengers who didn’t survive, out of a total of 887 passengers, so the gini impurity is as follows:\n",
    "\n",
    "```\n",
    "Gini = 2 * 342/887 * 545/887 = 0.4738\n",
    "```\n",
    "\n",
    "Again, here’s the first potential split.\n",
    "\n",
    "![](images/tree4.png)\n",
    "\n",
    "Note that we have 197+328=525 passengers on the left (Age<=30) and 145+217=362 passengers on the right (Age>30). Thus, pulling in the gini impurity values that we calculated before, we get the following information gain:\n",
    "\n",
    "```\n",
    "Information gain = 0.4738 - 525/887 * 0.4689 - 362/887 * 0.4802 = 0.0003\n",
    "```\n",
    "\n",
    "This value is very small meaning we gain very little from this split.\n",
    "\n",
    "Now let’s calculate the information gain for splitting on Sex.\n",
    "\n",
    "![](images/tree5.png)\n",
    "\n",
    "We have 233+81=314 passengers on the left (female) and 109+464=573 passengers on the right (male). Here is the information gain:\n",
    "\n",
    "```\n",
    "Information gain = 0.4738 - 314/887 * 0.3828 - 573/887 * 0.3081 = 0.1393\n",
    "```\n",
    "\n",
    "Thus we can see that the information gain is much better for this split. Therefore, splitting on Sex is a much better choice when building our decision tree than splitting on Age with threshold 30.\n",
    "\n",
    "> The work we did was just to compare two possible splits. We’ll need to do the same calculations for every possible split in order to find the best one. Luckily we don’t have to do the computations by hand!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building the Decision Tree\n",
    "\n",
    "We’ve built up the foundations we need for building the Decision Tree. Here’s the process we go through:\n",
    "\n",
    "To determine how to do the first split, we go over every possible split and calculate the information gain if we used that split. For numerical features like Age, PClass and Fare, we try every possible threshold. Splitting on the Age threshold of 50 means that datapoints with Age<=50 are one group and those with Age>50 are the other. Thus since there are 89 different ages in our dataset, we have 88 different splits to try for the age feature!\n",
    "\n",
    "We need to try all of these potential splits:\n",
    "1. Sex (male | female)\n",
    "2. Pclass (1 or 2 | 3)\n",
    "3. Pclass (1 | 2 or 3)\n",
    "4. Age (0 | >0)\n",
    "5. Age (<=1 | >1)\n",
    "6. Age (<=2 | >2)\n",
    "7. etc….\n",
    "\n",
    "There is 1 potential split for Sex, 2 potential splits for Pclass, and 88 potential splits for Age. There are 248 different values for Fare, so there are 247 potential splits for this feature. If we’re only considering these four features, we have 338 potential splits to consider.\n",
    "\n",
    "For each of these splits we calculate the information gain and we choose the split with the highest value.\n",
    "\n",
    "Now, we do the same thing for the next level. Say we did the first split on Sex. Now for all the female passengers, we try all of the possible splits for each of the features and choose the one with the highest information gain. We can split on the same feature twice if that feature has multiple possible thresholds. Sex can only be split on once, but the Fare and Age features can be split on many times.\n",
    "\n",
    "Independently, we do a similar calculation for the male passengers and choose the split with the highest information gain. Thus we may have a different second split for male passengers and female passengers.\n",
    "\n",
    "We continue doing this process until we have no more features to split on.\n",
    "\n",
    "> This is a lot of things to try, but we just need to throw computation power at it. It does make Decision Trees a little slow to build, but once the tree is built, it is very fast to make a prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "gini(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "A.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = [int(x) for x in input().split()]\n",
    "#A = [int(x) for x in input().split()]\n",
    "#B = [int(x) for x in input().split()]\n",
    "\n",
    "def log(function):\n",
    "    print(round(function(), 5))\n",
    "\n",
    "\n",
    "def gini(a, b):\n",
    "    p = a/(a+b)\n",
    "\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def inf_gain():\n",
    "    S1 = S.count(1)\n",
    "    S0 = S.count(0)\n",
    "    S_ = S1 + S0\n",
    "\n",
    "    A1 = A.count(1)\n",
    "    A0 = A.count(0)\n",
    "    A_ = A1 + A0\n",
    "\n",
    "    B1 = B.count(1)\n",
    "    B0 = B.count(0)\n",
    "    B_ = B1 + B0\n",
    "\n",
    "    HS = gini(S1, S0)\n",
    "    HA = gini(A1, A0)\n",
    "    HB = gini(B1, B0)\n",
    "\n",
    "    return HS - A_/S_*HA - B_/S_*HB\n",
    "\n",
    "#log(inf_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}